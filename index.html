<html>

	<head>

		<link rel="stylesheet" href="css/bootstrap.min.css" type="text/css"/>
    <link href="css/cover.css" rel="stylesheet">
    <link href="css/personal.css" rel="stylesheet">



    
    <link rel="stylesheet" href="jquery/css/smoothness/jquery-ui-1.8.2.custom.css" /> 
    <script type="text/javascript" src="jquery/js/jquery-1.11.2.min.js"></script> 
    <script type="text/javascript" src="jquery/js/jquery-ui-1.8.2.custom.min.js"></script> 
    <script type="text/javascript" src="js/scroll.js"></script>
  
    <link rel="stylesheet" type="text/css" href="css/slick.css">
  <link rel="stylesheet" type="text/css" href="css/slick-theme.css">

	  


    </script>
	</head>
	
	<title>Anthony Lowhur</title>
	
	<body>
	
		<div class="site-wrapper" id="wrapper">

      <div class="site-wrapper-inner" id="inner-wrapper">

        <div class="cover-container" id="cover">

          <div class="masthead clearfix" id="mast">
            <div class="inner" id="inner">

              <h3 class="masthead-brand">Anthony Lowhur</h3>
              <nav>
                <ul class="nav masthead-nav">
                  <li class="active"><a href="#">Home</a></li>
                  <li><a id ="projectlink" href="#project">Projects</a></li>
                  <li><a href="Lowhur-Anthony-Resume.pdf">Resume</a></li>
                </ul>
              </nav>
            </div>
          </div>
          <br><br><br>
          <div class="inner cover">
            <div class="roundedImage">&nbsp;</div>
            <h1 class="cover-heading">Anthony Lowhur</h1>
            <p class="lead">Programmer, Hacker, Aspiring Inventor</p>
            <p class="lead">
              <a href="#project" class="btn btn-lg btn-default">Click to learn more</a>
            </p>

          </div>
          <br>
     

          <div class="mastfoot">
            <div class="inner">
              <p>Feel free to say hi at antlowhur@yahoo.com</p>
            </div>
          </div>

        </div>
        
      </div>


    </div>
<div id="project" style="background-color:#084B8A;">
  <br><br><br><br><br><br>
    <div id="project" style="background-color:#424242; width:80%;margin-right:10%;margin-left:10%;">
    
            <br><br>
            <h1>Programming Projects</h1>
            <br><br>
            <div class="roundedImageAI">&nbsp;</div>
            <font size="5">Artificial Intellegence</font>
            <br><br>
            <br><br>
            AI Vocaloid Song Generator
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/mxqcCDOzUpk" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                This is program that uses deep learning to generate melodies and lyrics for Vocaloid songs (a vocaloid is a type of software that is used to generate music from synthesized voice). Uses transformer architecture to extract latent features and music structure from a set of vocaloid files and generate its own melodies. Used a custom markov model to generate lyrics under syllable count constraints to sync with number of notes from generated melody.  
                <br><br>
                

              </div>
            </center>

            <br><br>
            Trap shooting tracker and hit detector 
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/kFfQFqqoHSQ" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Made a program in OpenCV that tracks a flying clay pigeon disk and recognize whether a bullet hit the disk or missed. Is also able to detect the different type of hits it made (whether it was a solid hit, a decent hit [it splits into fragments], or a weak hit [change trajectory]).
                <br><br>
                

              </div>
            </center>


            <br><br>
            Mario AI via Deep Q-Learning
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/AZj5mX5Yfk4" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Decided to play around with reinforcement learning, so I implemented deep Q-learning and convolution neural networks using Pytorch in order to get an AI to play Mario and complete the first level (World 1-1) of Super Mario Bros. We take in pure pixel data from a Mario emulator (compatible with Open AI Gym) to feed into a CNN network and preform q-learning in the game.

                (sometimes Mario takes non-optimal actions which leds him to being stuck at tall pipes, but this will be improved in the future)
                <br><br>
                

              </div>
            </center>
            
            <br><br>
            One shot learning (image recogntion) on 10,856 unique Yugioh cards 
            <div class="lazy slider" data-sizes="50vw" style="height:440px;">
              <div>
                <br>
                 Small samples of recognized cards
                <img data-lazy="images/yuai_0.png" data-srcset="images/yuai_0.png 650w, images/yuai_0.png 960w" data-sizes="100vw">
                (Press the left or right arrows to change photos)
              </div>
              <div>
                <br>
                Comparing similarity of input image with dataset of cards (official art of cards). Left is input image while right are card arts from dataset.
                <img data-lazy="images/yuai_1.png" data-srcset="images/yuai_1.png 650w, images/yuai_1.png 960w" data-sizes="100vw">

              </div>
               <div>
                <br>
                Even if input image (left) was under very dark conditions, Dark Magician still gets recognized.
                <img data-lazy="images/yuai_2.png" data-srcset="images/yuai_2.png 650w, images/yuai_2.png 960w" data-sizes="100vw">
              </div>
               <div>
                <br>
                
                <img data-lazy="images/yuai_3.png" data-srcset="images/yuai_3.png 650w, images/yuai_3.png 960w" data-sizes="100vw">
              </div>
              <div>
                <br>
                Left card has card sleeve, right one is without
                <img data-lazy="images/yuai_4.png" data-srcset="images/yuai_4.png 650w, images/yuai_4.png 960w" data-sizes="100vw">
              </div>
              <div>
                Batch of different images under different contrast / lighting conditions. Left of each pair is input image, right is the card art from database
                <img data-lazy="images/yuai_5.png" data-srcset="images/yuai_5.png 650w, images/yuai_5.png 960w" data-sizes="100vw">
              </div>
              <div>
                <br>
                The AI classifier managed to achieve around 99% accuracy on all the cards in the game of Yugioh
                <img data-lazy="images/yuai_6.png" data-srcset="images/yuai_6.png 650w, images/yuai_6.png 960w" data-sizes="100vw">
              </div>
           
 
       
            </div>

            <center>
              <div style="width:70%">
                Made a machine learning classifier to recognize all 10,856 Yugioh cards in the game. Each card has a single card art associated with it, meaning each of the 10,856 classes had only 1 image to train on. To address these limitations, one shot learning was used by implementing triplet net based on ResNet50 along with triplet loss along with ORB algorithm for ranking support. Image augmentation and blur pooling was used to allow the classifier to achieve around 99% accuracy on all 10,856 cards under different lighting / contrast conditions. 
                <br><br>
                <a href = "https://www.reddit.com/r/yugioh/comments/i0m9dm/i_made_an_ai_that_recognizes_over_10000_yugioh/"  target="_blank"  style="color: #2E9AFE">
                  Click here to view more information about the Yugioh one-shot image classifier
                </a>
                <br>
		 <a href = "https://towardsdatascience.com/i-made-an-ai-to-recognize-over-10-000-yugioh-cards-26fc6aed1588/"  target="_blank"  style="color: #2E9AFE">
                  Click here to view more information about how it works and the process that went into making it.
                </a>
		<br>
		 <a href = "https://www.pyimagesearch.com/2021/03/03/an-interview-with-anthony-lowhur-recognizing-10000-yugioh-cards-with-computer-vision-and-deep-learning/"  target="_blank"  style="color: #2E9AFE">
                  Click here to view an interview about its development and implementation.
                </a>
                <br><br>
                

              </div>
            </center>
           

            <br><br>
            Face Tracking via Haar Classification and Lucas Kanade
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/42Snz1zzxUk" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Improved face tracking sample from OpenCV documentation by combining both Harr Classifcation and Lucas Kanade optical flow algorithm. This allowed the computer to track the face/head in various different angles (front face to side face) even with limited face dataset
                <br><br>
                

              </div>
            </center>
        
           

            <br><br>
            Trash Detection on the Beach for Autonomous Drones though Sand Segmentation
            <br><br>
            <a href="https://github.com/nkoplitz/BeachUAS" target="_blank"><img src= "images/sand_segment.jpg" width="560" height="315"></a>
            <br><br>
            <center>
              <div style="width:70%">
                Developped a script that would allow an autonomous drone to analyze a beach scene, detect trash on the beach, analyzing it based on certain properties of the trash (such as shape),and pick it up accordingly. Implemented backhistogram projection and morphological transform in order to segment out the the sand and detect the trash. Used the Bag of Words model to preform object recognition on each of the trash pieces (recognize between trash paper, water bottle, or plastic bag). This will be able to autonomate the process of picking up trash on the beach.
                <br><br>
                

              </div>
            </center>


            <br><br>
            Pick up robot for Amazon Robotics Challenge
            <br><br>

            <img src= "images/amazon0.png" width="360" height="315"></a>
            <br>
            <img src= "images/amazon1.jpg" width="320" height="215"></a><img src= "images/amazon3.png" width="360" height="215"></a>
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/v63rYOxZaDc" frameborder="0" allowfullscreen></iframe>

            <br><br>
            <center>
              <div style="width:70%">
                 Designeing intelligence of an Amazon Picking Challenge robot through computer vision and machine learning. Implemented 2D object recognition with convolution neural networks. Currently researching in effective image segmentation algorithms for object localization. Leading team of industrial engineering students (senior undergraduate + master graduates) for the development of the autonomous robot. Attempting to lead the team to be the first Rutgers team to enter nationals of the Amazon competition.
                <br><br>
                

              </div>
            </center>

            <br><br>
            Charm City Murals (mural detector)
            <br><br>
            
              
              <img src= "images/hophacks1.png" width="460" height="200"></a>     
              <img src= "images/hophacks2.png" width="460" height="200"></a>
              <br><br>
              <img src= "images/hophacks0.png" width="500" height="280"></a>
              

            <br><br>
            <center>
              <div style="width:70%">
                This program uses canny edge detection, morphological transform, and convolutional neural networks to detect and recognize historical murals on various walls in Baltimore. After it recognizes the mural, it would give information about the mural's history and origin. Also preform perspective transform if one wants to get a better, more birds eye view of the mural. 
                <br><br>
                Won the 2nd place in HopHacks Fall 2018 hackathon, along with Google Cloud API and "Most-Baltimore" prize.
                <br><br>
                

              </div>
            </center>

            <br><br>
            Fire Detector and Analyzer
            <br><br>
            
             <iframe width="560" height="315" src="https://www.youtube.com/embed/IRmoD1BJFHM" frameborder="0" allowfullscreen></iframe>
            <br><br>
             <iframe width="560" height="315" src="https://www.youtube.com/embed/HXLc0rLK6_g" frameborder="0" allowfullscreen></iframe>

            <br><br>
            <center>
              <div style="width:70%">
                This program uses computer vision searches for fires by searching for intense brightness and fire-like movement in the video. It then monitors the rate of growth and current size of the fire to determine on how threatening a fire is becoming and gives a pre-emptive warning (via phone notification) to nearby individuals if it gets to a certain size / surpasses a certain growth rate.


                <br><br>
                This was designed for PennApps Fall 2018 where it placed in top 30 and won the DocuSign API prize.
                <br><br>
                

              </div>
            </center>

            <br><br>
            Tactile Vision Shirt
            <br><br>
            <img src= "images/medhacks0.jpg" width="20%" height="auto"></a>     
            <img src= "images/medhacks4.jpg" width="20%" height="auto"></a>
            <br><br>
            <img src= "images/medhacks1.jpg" width="25%" height="auto"></a>    
            <img src= "images/medhacks2.jpg" width="35%" height="auto"></a>
            <img src= "images/medhacks3.jpg" width="25%" height="auto"></a>
            

            <br><br>
            <center>
              <div style="width:70%">
                Made a hardware hack that recreated the sense of sight to the blind through the use of tactile touch. Based on the concept of graphesthesia, this project utilized a grid of 5 x 5 vibration motors sewed on the back of a shirt and two webcams to locate the object in order to vibrate the object's 3D location on the user's back. Implemented depth maps through stereo vision and object detection through YOLO neural network architecture (MobileNet) to be used on a Raspberry Pi. Also designed a downsampling algorithm to preserve semantic and spatial information from a webcam with 300x400 resolution to 5x5 resolution for the vibration motor grid.


                <br><br>
                Won 1st place overall at MedHacks Fall 2019, along with winning the Global Management of Chronic Disease track.
                <br><br>
                

              </div>
            </center>


          <br><br>
           Automatic decision-based workflow generator via speech processing (Autoflow)
            <br><br>
            
              <a href="https://devpost.com/software/auto-flow" target="_blank"><img src= "images/autoflow0.png" width="35%"><img src= "images/autoflow1.jpeg" width="30%">
                <br>
                <img src= "images/autoflow3.png" width="60%">
                <br><br>
                <img src= "images/autoflow2.png" width="60%"></a>

            <br><br>
            <center>
              <div style="width:70%">
                Made an app that can create an automated workflow of APIs produced through spoken speech. This project takes in a spoken query via speech to text and uses the query to build an automated decision tree that handles the workflow's use various types of APIs (via Standard Library) under dynamic and conditional situations. Created an algorithm that turns user's input queries into a automated decision tree through parts of speech tagging, as well as designed an algorithm that detect redundant queries and workflows by using cosine similarity. Also helped designed the algorithm that represents the automated workflow tree in the form of JSON files.
                <br><br>
                Placed in the finalist round at HackPrinceton Fall 2019 as well as winning the Standard Library prize.
                <br><br>
                

              </div>
            </center>
            <br><br>
            Deepfake algorithm (CycleGANs)

            
            <div class="lazy slider" data-sizes="50vw" style="height:410px;">
              <div>
                <br><br><br>
                Obama's face swapped with Trump's
                <img data-lazy="images/df_0.jpg" data-srcset="images/df_0.jpg 650w, images/df_0.jpg 960w" data-sizes="100vw">
                (Press the left or right arrows to change photos)
              </div>
              <div>
                <br><br><br>
                Trump's face swapped with Putin's
                <img data-lazy="images/df_1.jpg" data-srcset="images/df_1.jpg 650w, images/df_1.jpg 960w" data-sizes="100vw">

              </div>
               <div>
                <br><br>
                Trump's face swapped with Putin's
                <img data-lazy="images/df_2.jpg" data-srcset="images/df_2.jpg 650w, images/df_2.jpg 960w" data-sizes="100vw">
              </div>
               <div>
                <br><br><br>
                Putin's face swapped with Trump's
                <img data-lazy="images/df_3.jpg" data-srcset="images/df_3.jpg 650w, images/df_3.jpg 960w" data-sizes="100vw">
              </div>
              <div>
                <br><br><br>
                Obama's face swapped with Trump's
                <img data-lazy="images/df_4.jpg" data-srcset="images/df_4.jpg 650w, images/df_4.jpg 960w" data-sizes="100vw">
              </div>
              <div>
                A girl's face (Shuka) swapped with Trump's
                <img data-lazy="images/df_5.jpg" data-srcset="images/df_5.jpg 650w, images/df_5.jpg 960w" data-sizes="100vw">
              </div>
              <div>
                <br><br><br>
                A girl's face (Shuka) swapped with Trump's
                <img data-lazy="images/df_6.jpg" data-srcset="images/df_6.jpg 650w, images/df_6.jpg 960w" data-sizes="100vw">
              </div>
              <div>
                Arisa's face swapped with Shuka's
                <img data-lazy="images/df_7.jpg" data-srcset="images/df_7.jpg 650w, images/df_7.jpg 960w" data-sizes="100vw">
              </div>
              <div>
                Arisa's face swapped with Shuka's
                <img data-lazy="images/df_11.jpg" data-srcset="images/df_11.jpg 650w, images/df_11.jpg 960w" data-sizes="100vw">
              </div>
              <div>
                Shuka's face swapped with Arisa
                <img data-lazy="images/df_9.jpg" data-srcset="images/df_9.jpg 650w, images/df_9.jpg 960w" data-sizes="100vw">
              </div>
              <div>
                <br><br><br>
                Arisa's face swapped with Shuka's
                <img data-lazy="images/df_10.jpg" data-srcset="images/df_10.jpg 650w, images/df_10.jpg 960w" data-sizes="100vw">
              </div>
 
       
            </div>


            <center>

              <div style="width:70%;">
                Made an AI that can transform the face of anyone into image with another person's face (in this case, Obama -> Trump and Trump -> Putin). The algorithm was implemented through a CycleGAN to change the image domain of facial data along affline warping and transformation to extract the original face and replace with a generated face of another subject. Extracted training data using a web scaper along with implementing several different image preproccessing scripts. 
                

              </div>
            </center>

            </center>


             <br><br>
             Terms Of Service Highlighter (TOSH)



            <div class="lazy slider" data-sizes="50vw" style="height:410px;">
              <div>
                <br><br>
                Form to allow users to write a complaint
                <img src= "images/TOSH-img4.png" width="100%" height="auto">
                (Press the left or right arrows to change photos)
              </div>
              <div>
                <br><br>
                Hightlighted sections of a terms of service document
                <img src= "images/TOSH-img0.png" width="100%" height="auto">
                
              </div>
              
              <div>
                <br><br>
                Sample terms of service document
                <img src= "images/TOSH-img5.png" width="100%" height="auto">
                
              </div>
              <div>
                <br><br>
                Results from using complaints and terms of service from Hotels.com
                <img src= "images/TOSH-img1.jpg" width="100%" height="auto">
                
              </div>
              <div>
                <br><br>
                Results from using complaints and terms of service from Retro Fitness
                <img src= "images/TOSH-img3.jpg" width="100%" height="auto">
                
              </div>

            </div>

            <center>
            
              <div style="width:70%">
                This web app uses natural language processing in order to highlight the most relavent and complained about sections in a terms of service document. This is accomplished by taking a list of complaints, preform text-processing to filter out stop words, calculating the frequency-term matrix of the complaints and sections of terms of service, and perform cosine-simularity between complaints and terms of service section. This allows us to get a ranked list of relevent sections of terms of service to warn users of possible texts in a contract that would be crucial to read. Made using Python NTLK and Python Flask.
                <br><br>
                Won the "Best Hack for the NYC community" at TechFestival 2019.
                <br><br>
                

              </div>
            </center>

              <br><br>
            Dense Optical Flow based Emotion Recognition System
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/pPclypFDcrk" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Worked with a Rapiro robot by programming a computer vision program such as an emotion recognition system based on facemovements using Dense Optical Flow and trained it by using Support Vector Machines. Takes advantage of facial movements lack of vulnerability to lighting and unique facial appearance that an emotion classifier would havewith just static images, creating a more accurate and robust emotion classifier. 
                <br><br>
                Wrote a research paper and presented and published it as 1st author at the 2015 IEEE 12th International Conference (MASS) workshop in Dallas, Texas. 
                <br>
                <a href='http://ieeexplore.ieee.org/document/7366995/?reload=true&arnumber=7366995' target="_blank"  style="color: #2E9AFE">You can read it here</a>
                <br><br>
                (Note: the video above shows that the program shows the wrong confidence score information. This was fixed after the video was recorded.)
                <br><br>
                <a href="https://www.youtube.com/watch?v=PM_k_zACWLE" target="_blank"  style="color: #2E9AFE">Click here to see the program implemented in the Raspberry Pi of the Rapiro Robot</a>
              </div>
            </center>



             <br><br>
            CNN-LSTM Video Classifier 
            <br><br>
            
              <iframe width="560" height="315" src="https://www.youtube.com/embed/yzEKe9Eoa0Q" frameborder="0" allowfullscreen></iframe>

            <br><br>
            <center>
              <div style="width:70%">
               This video was a result of training a CNN-LSTM video classifer on a small dataset. By analyzing a series of frames in each video, the classifer is able to recognize actions going on in the video. 

		By having CNN encoder taking in the frames, the frames are parsed to be analyzed by the LSTM which analyzes the temporial structure of the frames and decodes the output to get a classification of the video.  

              </div>
            </center>



             <br><br>
            Video Panorama Generator 
            <br><br>
            
              <iframe width="560" height="315" src="https://www.youtube.com/embed/hVynpc-li1w" frameborder="0" allowfullscreen></iframe>

            <br><br>
            <center>
              <div style="width:70%">
                This video is a result from a python script I made that stitches two videos that were recorded from two cameras looking at the same scene at different angle to create video panoramas. Uses the SIFT algorithm to find similar points and calculate a transformation matrix with OpenCV to stitch the videos to together
                This video is the end result. May work on improving making the seams less visible when I get the time.
                

              </div>
            </center>


            <br><br>
            Surround and Capturing Adversarial Agents through Decentralized Multi-Agent Intelligence
            <br><br>
            
              <a href="TTUResearch0.pdf"><img src= "images/swarm0.png" width="440" height="340"></a>

            <br><br>
            <center>
              <div style="width:70%">
                Designed a multi-agent intelligence algorithm (swarm intelligence) where a team of ally agents work together to surround and capture a fleeing adversarial agent. This allowed agents to search for interest points to pursue and surround enemy agent with individual behavior, but minimal communication to work together as an agent team for capturing an adversarial agent.
                <br><br>
                Abstract was accepted to the National Conference On Undergraduate Research (NCUR 2017) at the Memphis, Tennessee.
                <br><br>
                <a href="TTUResearch0.pdf" target="_blank" style="color: #2E9AFE">You can see the research poster here</a>
                <br><br>
                

              </div>
            </center>


            <br><br>
            Drawing to Website Generator using Computer Vision
            <br><br>
            
              <iframe width="560" height="315" src="https://www.youtube.com/embed/7QbGQF6JS4g" frameborder="0" allowfullscreen></iframe>

            <br><br>
            <center>
              <div style="width:70%">
                Made an AI website generator that uses computer vision to recognize shapes and position formats of your drawing and turns it into your own website. You would draw a picture of your website, upload your drawing, specify the topic of your website, and the program will extract information from multiple sources about that topic and use the information to populate your generated website.
 
              </div>
            </center>


            <br><br>
            White Space detector



            <div class="lazy slider" data-sizes="50vw" style="height:410px;">
              <div>
                <br><br><br>
                <img src= "images/pru0.png" width="45%" height="auto">
                (Press the left or right arrows to change photos)
              </div>
              <div>
                <br><br><br>
                <img src= "images/pru1.png" width="45%" height="auto">
                
              </div>
              <div>
                <br><br><br>
                <img src= "images/pru2.png" width="45%" height="auto">
                
              </div>
              <div>
                <br><br><br>
                <img src= "images/pru3.png" width="45%" height="auto">
                
              </div>
              <div>
                <br><br><br>
                <img src= "images/pru4.png" width="45%" height="auto">
                
              </div>
              <div>
                <br><br><br>
                <img src= "images/pru5.png" width="45%" height="auto">

                
              </div>
            </div>

            <center>
            
              <div style="width:70%">
                Implemented canny edge detection and morphological transform in order to search for whitespace or less detailed spaces with an image to write informative headlines or text on to it. Removes the need for content writers to go through endless of photos and manually edit and add text, saving content writers a lot of time.
                <br><br><br>
                (This was made for a company, and permission was given to post these)
                <br><br>
                

              </div>
            </center>

            <br><br>
            AI Melody Generator
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/ImygJueWKQE" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Created a melody generator that takes in a song from a midi, extracting musical aspects of song, recognizing patterns of the musical structure by using Long Short Term Memory(LSTM) Neural Network, and composing its own original melodies based on the patterns it had learned. 
                <br><br>
                

              </div>
            </center>

            <br><br>
            Hand-drawn graphs to LATEX translator
            <br><br>
            
              <a href="https://github.com/aliang8/Paper2LaTeX" target="_blank"><img src= "images/graph0.png" width="340" height="200"><img src= "images/graph1.png" width="340" height="200"></a>

            <br><br>
            <center>
              <div style="width:70%">
                Implented computer vision and machine learning in order to analyze a hand-drawn graph on paper, seach and analyze shapes such as nodes and lines, and translate them into a more tidy and neat LATEX graph. Helped plan computer vision algorithms to detect nodes and lines while worked on optical character recognitiion and arrow direction detection.
                <br><br>
                Placed in top 30 hacks in PennApps Winter 2017 hackathon.
                <br><br>
                

              </div>
            </center>

            <br><br>
            Road Segmentation For Autonomous Vehicles
            <br><br>
            <a href="https://github.com/vanstorm9/urban-road-recognition" target="_blank"><img src= "images/road_0.png" width="560" height="270"></a>
            <br><br>
            <center>
              <div style="width:70%">
                Designed a simple script that uses histogram backprojection along with morphological transform in order detect a road in a scene despite any large amounts of noise in the picture. Made to be implemented in the DriveAI project, an initiative for open-source autonomous vehicles.
                <br><br>
                

              </div>
            </center>

             <br><br>
            Lyrics Generator Based on Markov Chains
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/O0VlHfLAVzs" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Implemented markov chains on lyrics data scrapped from the web to create program that can generate its own unique set of lyrics based on genre. Also have the capability of creating lyrics based on syllable count per line. Part of a long term project to create a full AI song composer. 
                <br><br>
                

              </div>
            </center>
            <!--
            <br><br>
            Rutgers Navigation System
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/Pt3fnORMzA8" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Uses the A* algorithm in order to find the shortest path from point A to point B within Rutgers campus (to find buildings, bus stops, dining halls, etc) (currently Busch campus) as well as kinematics equations to find the time of the walk. Calcuates both the path and information of bus route and walking route to help one determine the more efficient way of getting to a destination (takes into account the bus route, average walking and bus driving speed, and distance between each places)
                <br><br>
                

              </div>
            </center>
            -->

            <br><br><br><br><br><br><br><br><br><br><br><br>
            <div class="roundedImageHCI">&nbsp;</div>
            <font size="5">Human-Computer Interaction</font>
            <br><br>
            <br><br>
            Yugioh Duel Disk (using image recongition)
            <div class="lazy slider" data-sizes="50vw">
              <div  style="height:400px;">
                <br>
                Video demo of the duel disk
                <video  width="100%" width="auto" poster="images/yudd_5.jpg" controls>
                  <source src= "video/yudd_0.mp4" type="video/mp4">
                </video>
                (Press the left or right arrows to change photos)
              </div>
              <div style="height:410px;">
                <br><br><br><br><br><br>
                Set up of prototype duel disk
                <img data-lazy="images/yudd_5.jpg" data-srcset="images/yudd_5.jpg 650w, images/yudd_5.jpg 960w" data-sizes="100vw">
             
              </div>
              <div>
                <br><br><br><br><br><br>
                Touch screen interface
                <img data-lazy="images/yudd_2.jpg" data-srcset="images/yudd_2.jpg 650w, images/yudd_2.jpg 960w" data-sizes="100vw">
              </div>
              <div>
                <br>
                Image recognition on one of the cards
                <img data-lazy="images/yudd_8.jpg" data-srcset="images/yudd_8.jpg 650w, images/yudd_8.jpg 960w" data-sizes="100vw">
              </div>
               <div>
                <br>
                Jetson Nano used for this project (picture at beginning of testing)
                <img data-lazy="images/yudd_0.jpg" data-srcset="images/yudd_0.jpg 650w, images/yudd_0.jpg 960w" data-sizes="100vw">
              </div>
              <div>
                <br><br><br><br><br><br>
                When dueling with someone far away, you are able to view and read descriptions of cards on the field
                <img data-lazy="images/yudd_3.png" data-srcset="images/yudd_3.png 650w, images/yudd_3.png 960w" data-sizes="100vw">
              </div>
               
              <div>
                <br><br><br><br><br><br>
                Picture of the two duel disks
                <img data-lazy="images/yudd_6.jpg" data-srcset="images/yudd_6.jpg 650w, images/yudd_6.jpg 960w" data-sizes="100vw">
              </div>
              <div>
                
                <img data-lazy="images/yudd_7.jpg" data-srcset="images/yudd_7.jpg 650w, images/yudd_7.jpg 960w" data-sizes="100vw">
              </div>
           
 
       
            </div>

            <center>
              <div style="width:70%">
                Made a Yugioh duel disk, a device that allows one to play the card game Yugioh at a distance away. The device is powered by a Jetson Nano (can also be powered by Raspberry Pi) connected to a camera and screen. It uses a machine learning model that I trained on 10,856 unique cards via one shot learning to recognize physical cards through a camera and allows the player to use the card in game. Also capable of network / communication capabilities by using sockets in Python to have two duel disks communicate with each other about information and game state changes. Python Flask was used as the interface to interact with the duel disk.
                <br><br>
                <a href = "https://www.reddit.com/r/yugioh/comments/idagc9/i_made_a_functional_duel_disk_powered_by_ai/"  target="_blank"  style="color: #2E9AFE">
                  Click here to view more information about the Yugioh Duel Disk
                </a>
                <br><br>
                

              </div>
            </center>
            <br><br>
            Force VR Gauntlet
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/PMwATLDO3yU" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Used a motor (with rope tethered to arm) controlled by the Arduino to make a virtual reality wearable that allows the wearer to experience physical forces. Whenever the user moves / swings his arm in the VR game and a hit is detected, the Arduino recieves that information and pulls on the arm with the motor tethered to arm, creating a locking effect and imitation of a physical force. It was designed to solve the problem of lack of physical forces in virtual reality (say you have a sword. You hit an object and your arm stops because it cannot go through an object, but in VR, your arm would simply phase through the object). With this developped a little bit more, actual sword combat will be finally possible in virtual reality.
                <br><br>
              </div>
            </center>

            <br><br>
            Kinect Helper
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/svzljeO0i2k" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Used the Kinect to create a workspace that is able to sense the presence of its owner. By using object detection, the
                Kinect is able to call a bash script that gives the Kinect control of the computer based on user input, creating a smart workspace. In this case the Kinect can sense when you get up from your chair and will automatically shut off the screen, this saves power and is more convenient than having to do it manually. We also included other features such as the ability to change the brightness of the screen using a hand held up to the sensor. Any other features can be added to the program and controlled with a hand held up to the Kinect.
                <br><br>
                
              </div>
            </center>

            <br><br>
            Leaptop
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/SqsctnDnpdc" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Used the Leap Motion in order to control some computer functions using just hand gestures. This includes opening up a browser, closing a browser, putting the laptop to sleep, and locking it using hand movements. This was made in Python which was also controlling a bash script.
                <br><br>
              </div>
            </center>
            <br><br>
            Hologram Pyramid CAD program
            <br><br>

            <iframe width="560" height="315" src="https://www.youtube.com/embed/Kmb2xlh2wm8" frameborder="0" allowfullscreen></iframe><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/1y1NO4AM0k4" frameborder="0" allowfullscreen></iframe>

            <br><br>
            <center>
              <div style="width:70%">
                
                EDIT: I know, the quality of the video isn't that good, so I will upload another video in the near future. <br><br>

              A friend and I made a projector that uses pepper's ghost to create a hologram like image in an acrylic pyramid. Used Unity for the models and projection scene along with Leap Motion to give us the ability to control the orientation of the model. If developped more, it would serve as a CAD program for designers, artists, and engineers so that they can view their models and control them in 3D space.
                <br><br>
              </div>
            </center>
            <br><br>
            Hologram Emotion Face Control
            <br><br>

            <a href="https://github.com/megatran/EmotionRecognitionOnHologram" target="_blank"><img src= "images/emotionPyramid.jpg" width="560" height="315"></a>

            <br><br>
            <center>
              <div style="width:70%">
                
              We use the hologram pyramid to project a face into the pyramid. By using convolution neural networks and a server, we were able to change the the hologram face's emotion from happy to angry depending on what the emotion of someone else on another computer. We hope to work on full face control in the future.
                <br><br>
              </div>
            </center>
            <br><br>
            <font size="5">Game Development</font>
            <br><br>

            Myo Multiplayer Sword Fighting Game
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/6phNDQvRKuU" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Created a sword fighting multiplayer game wheremultiple people can log into the same server and engage sword combat with each other. Programmedthe Myo so that users are able to preform sword attacks through sword cutting like hand gestures. It was designed as a test game that may be implemented with the Force VR Gauntlet.
                <br><br>
              </div>
            </center>
            <br><br>
            Twitch Based Jenga Multiplayer Game
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/v_voaCJqpFA" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Created a Twitch based collaboration game in Jenga where players can log into the same server and play Jenga with each other while the audience is able to interfer with the gameplay in Unity by tying in commands on Twitch. Created this as my first Unity game development project.
                <br><br>
              </div>
            </center>
             <br><br>
            NES Zelda recreaton
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/StfCZy4-NRA" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                A game I made in Visual Basic, which is a small segment recreation of the Legend of Zelda for NES.
              </div>

            <br><br>
            <font size="5">Web Development</font>
            

            <br><br>
            Dodo-Matchmaker 
            <div class="lazy slider" data-sizes="50vw" style="height:410px;">
              <div>
                <br><br>
                Quick matchup feature demo
                <video  width="100%" height="auto" controls>
                  <source src= "video/dodo-0.mp4" type="video/mp4">
                </video>
                (Press the left or right arrows to change photos)
              </div>
              <div>
                <br><br>
                Private host room feature demo
                <video  width="100%" height="auto" controls>
                  <source src= "video/dodo-1.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <center>
              <div style="width:70%">
                An online code matchmaker website designed for players of the game, "Animal Crossing: New Horizons". Matches different players by distributing online access codes (dodo-code) to different players based on specific conditions. Also contains Facebook verification to help improve responoses to and discourage scams online in the game. Made in Python Flask, socketio, Javascript, HTML, and CSS.

              </div>


            <br><br>
            Animal Crossing GIF Storymaker
            <br><br>
            <video  height="380px" width="auto" controls>
              <source src= "video/gif-demo.mp4" type="video/mp4">
            </video>
            <br><br>
            <center>
              <div style="width:70%">
                A web app designed for the game "Animal Crossing: New Horizons". Allows you to generate your own cutscenes based on a particular scene of the character, Isabelle, doing daily morning annoucnements. Uses frames from in game and allows you to mix different reactions in different combinations along with dialogue text customization. Extracted frames from the game and pre-processed using Python before being used in the app.
                <br><br>
                <a href="http://ac-storymaker.github.io/" target="_blank"  style="color: #2E9AFE">Check out the site</a>
              </div>
            <br><br>

            Appblox
            <div class="lazy slider" data-sizes="50vw" style="height:410px;">
              <div>
                <br><br>
                Sample of a generated app
                <img data-lazy="images/appblox0.png" width="100%" height="auto"  data-srcset="images/appblox0.png 650w, images/appblox0.png 960w" data-sizes="100vw">
                (Press the left or right arrows to change photos)
              </div>
              <div>  
                <br><br>
                Sample diagram of a sample app one can make
                <img data-lazy="images/appblox5.png" width="100%" height="auto" data-srcset="images/appblox5.png 650w, images/appblox5.png 960w" data-sizes="100vw">
                
              </div>
              <div>
                <br><br>
                Appblox app creation interface
                <img data-lazy="images/appblox1.png" width="100%" height="auto"  data-srcset="images/appblox1.png 650w, images/appblox1.png 960w" data-sizes="100vw">

              </div>
              <div>
                <br><br>
                Generated webapp to take in an image
                <img data-lazy="images/appblox2.png" width="100%" height="auto"  data-srcset="images/appblox2.png 650w, images/appblox2.png 960w" data-sizes="100vw">

              </div>
              <div>
                <br>
                Generated texting along with reporting how many faces in image
                <img data-lazy="images/appblox4.png" width="100%" height="auto"  data-srcset="images/appblox4.png 650w, images/appblox4.png 960w" data-sizes="100vw">

              </div>
              

            </div>


            <center>
              <div style="width:70%">
                A site that allows you to create your own web app in a matter of seconds. It does this by hosting a number of API services to serve as module blocks and giving the user the ability to mix and mash them together in different permuatations to create apps of different functionality. Output of one module block will serve as input to the next module block. Implemented by using Python Flask with APIs such as Stdlib, Microsoft Azure, and Twillio.

              </div>






        





            <br><br>
            WannaStudy
            <br><br>
            <center>
              <div style="width:70%">
                A social networking forum designed to help create academic community and supports the circulation of academic information within colleges in order for students to ask and answer questions, form study groups, study online through Google Hangouts (button integrated on site), and share information.
              </div>
            <br><br>
            Rutgers Roommate Search Engine
            <br><br>
            <center>
              <div style="width:70%">
                Account system website designed to have users register themselves to a roommate search engine in order to help them or others to find the perfect roommate. Includes an advanced search section, a list of roommates filtered through advance search, profile pages for each user, and roommate lists to add potential roommates on their list. 
                
              </div>
            <br><br>
            PVHS GPA Calculator
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/to74NTcSUtE" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                An old website with PHP/SQL account systems that helps students calculate their GPA (marking period, end-of-year, and current)as well final grad. This was made when I was a high school student.
              </div>
            <br><br>


            <br><br><br><br><br><br><br><br><br><br>
          </div>
	 </div>
	
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
		<script src = "js/bootstrap.js"></script>
  <script src="js/slick.js" type="text/javascript" charset="utf-8"></script>

	</body>


	
</html>
