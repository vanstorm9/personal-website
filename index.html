<html>

	<head>

		<link rel="stylesheet" href="css/bootstrap.min.css" type="text/css"/>
    <link href="css/cover.css" rel="stylesheet">
    <link href="css/personal.css" rel="stylesheet">



    
    <link rel="stylesheet" href="jquery/css/smoothness/jquery-ui-1.8.2.custom.css" /> 
    <script type="text/javascript" src="jquery/js/jquery-1.11.2.min.js"></script> 
    <script type="text/javascript" src="jquery/js/jquery-ui-1.8.2.custom.min.js"></script> 
    <script type="text/javascript" src="js/scroll.js"></script>
  
    <link rel="stylesheet" type="text/css" href="css/slick.css">
  <link rel="stylesheet" type="text/css" href="css/slick-theme.css">

	  


    </script>
	</head>
	
	<title>Anthony Lowhur</title>
	
	<body>
	
		<div class="site-wrapper" id="wrapper">

      <div class="site-wrapper-inner" id="inner-wrapper">

        <div class="cover-container" id="cover">

          <div class="masthead clearfix" id="mast">
            <div class="inner" id="inner">

              <h3 class="masthead-brand">Anthony Lowhur</h3>
              <nav>
                <ul class="nav masthead-nav">
                  <li class="active"><a href="#">Home</a></li>
                  <li><a id ="projectlink" href="#project">Projects</a></li>
                  <li><a href="Lowhur-Anthony-Resume.pdf">Resume</a></li>
                </ul>
              </nav>
            </div>
          </div>
          <br><br><br>
          <div class="inner cover">
            <div class="roundedImage">&nbsp;</div>
            <h1 class="cover-heading">Anthony Lowhur</h1>
            <p class="lead">Programmer, Hacker, Aspiring Inventor</p>
            <p class="lead">
              <a href="#project" class="btn btn-lg btn-default">Click to learn more</a>
            </p>

          </div>
          <br>
     

          <div class="mastfoot">
            <div class="inner">
              <p>Feel free to say hi at antlowhur@yahoo.com</p>
            </div>
          </div>

        </div>
        
      </div>


    </div>
<div id="project" style="background-color:#084B8A;">
  <br><br><br><br><br><br>
    <div id="project" style="background-color:#424242; width:80%;margin-right:10%;margin-left:10%;">
    
            <br><br>
            <h1>Programming Projects</h1>
            <br><br>
            <div class="roundedImageAI">&nbsp;</div>
            <font size="5">Artificial Intellegence</font>
            <br><br>
            <br><br>
            AI Melody Generator
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/ImygJueWKQE" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Created a melody generator that takes in a song from a midi, extracting musical aspects of song, recognizing patterns of the musical structure by using Long Short Term Memory(LSTM) Neural Network, and composing its own original melodies based on the patterns it had learned. Part of a long term project to create a full AI song composer, an AI that can generate entire songs with a series melodies.
                <br><br>
                

              </div>
            </center>

             <br><br>
            Dense Optical Flow based Emotion Recognition System
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/pPclypFDcrk" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Worked with a Rapiro robot by programming a computer vision program such as an emotion recognition system based on facemovements using Dense Optical Flow and trained it by using Support Vector Machines. Takes advantage of facial movements lack of vulnerability to lighting and unique facial appearance that an emotion classifier would havewith just static images, creating a more accurate and robust emotion classifier. 
                <br><br>
                Wrote a research paper and presented and published it as 1st author at the 2015 IEEE 12th International Conference (MASS) workshop in Dallas, Texas. 
                <br>
                <a href='http://ieeexplore.ieee.org/document/7366995/?reload=true&arnumber=7366995' target="_blank"  style="color: #2E9AFE">You can read it here</a>
                <br><br>
                (Note: the video above shows that the program shows the wrong confidence score information. This was fixed after the video was recorded.)
                <br><br>
                <a href="https://www.youtube.com/watch?v=PM_k_zACWLE" target="_blank"  style="color: #2E9AFE">Click here to see the program implemented in the Raspberry Pi of the Rapiro Robot</a>
              </div>
            </center>

            <br><br>
            Trap shooting tracker and hit detector 
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/kFfQFqqoHSQ" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Made a program in OpenCV that tracks a flying clay pigeon disk and recognize whether a bullet hit the disk or missed. Is also able to detect the different type of hits it made (whether it was a solid hit, a decent hit [it splits into fragments], or a weak hit [change trajectory]).
                <br><br>
                

              </div>
            </center>


            <br><br>
            Mario AI via Deep Q-Learning
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/AZj5mX5Yfk4" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Decided to play around with reinforcement learning, so I implemented deep Q-learning and convolution neural networks using Pytorch in order to get an AI to play Mario and complete the first level (World 1-1) of Super Mario Bros. We take in pure pixel data from a Mario emulator (compatible with Open AI Gym) to feed into a CNN network and preform q-learning in the game.

                (sometimes Mario takes non-optimal actions which leds him to being stuck at tall pipes, but this will be improved in the future)
                <br><br>
                

              </div>
            </center>


            <br><br>
            Face Tracking via Haar Classification and Lucas Kanade
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/42Snz1zzxUk" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Improved face tracking sample from OpenCV documentation by combining both Harr Classifcation and Lucas Kanade optical flow algorithm. This allowed the computer to track the face/head in various different angles (front face to side face) even with limited face dataset
                <br><br>
                

              </div>
            </center>
        
            <br><br>
            Lyrics Generator Based on Markov Chains
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/O0VlHfLAVzs" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Implemented markov chains on lyrics data scrapped from the web to create program that can generate its own unique set of lyrics based on genre. Also have the capability of creating lyrics based on syllable count per line. Part of a long term project to create a full AI song composer. 
                <br><br>
                

              </div>
            </center>

            <br><br>
            Trash Detection on the Beach for Autonomous Drones though Sand Segmentation
            <br><br>
            <a href="https://github.com/nkoplitz/BeachUAS" target="_blank"><img src= "images/sand_segment.jpg" width="560" height="315"></a>
            <br><br>
            <center>
              <div style="width:70%">
                Developped a script that would allow an autonomous drone to analyze a beach scene, detect trash on the beach, analyzing it based on certain properties of the trash (such as shape),and pick it up accordingly. Implemented backhistogram projection and morphological transform in order to segment out the the sand and detect the trash. Used the Bag of Words model to preform object recognition on each of the trash pieces (recognize between trash paper, water bottle, or plastic bag). This will be able to autonomate the process of picking up trash on the beach.
                <br><br>
                

              </div>
            </center>


            <br><br>
            Pick up robot for Amazon Robotics Challenge
            <br><br>

            <img src= "images/amazon0.png" width="360" height="315"></a>
            <br>
            <img src= "images/amazon1.jpg" width="320" height="215"></a><img src= "images/amazon3.png" width="360" height="215"></a>
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/v63rYOxZaDc" frameborder="0" allowfullscreen></iframe>

            <br><br>
            <center>
              <div style="width:70%">
                 Designeing intelligence of an Amazon Picking Challenge robot through computer vision and machine learning. Implemented 2D object recognition with convolution neural networks. Currently researching in effective image segmentation algorithms for object localization. Leading team of industrial engineering students (senior undergraduate + master graduates) for the development of the autonomous robot. Attempting to lead the team to be the first Rutgers team to enter nationals of the Amazon competition.
                <br><br>
                

              </div>
            </center>

            <br><br>
            Charm City Murals (mural detector)
            <br><br>
            
              
              <img src= "images/hophacks1.png" width="460" height="200"></a>     
              <img src= "images/hophacks2.png" width="460" height="200"></a>
              <br><br>
              <img src= "images/hophacks0.png" width="500" height="280"></a>
              

            <br><br>
            <center>
              <div style="width:70%">
                This program uses canny edge detection, morphological transform, and convolutional neural networks to detect and recognize historical murals on various walls in Baltimore. After it recognizes the mural, it would give information about the mural's history and origin. Also preform perspective transform if one wants to get a better, more birds eye view of the mural. 
                <br><br>
                Won the 2nd place in HopHacks Fall 2018 hackathon, along with Google Cloud API and "Most-Baltimore" prize.
                <br><br>
                

              </div>
            </center>

            <br><br>
            Fire Detector and Analyzer
            <br><br>
            
             <iframe width="560" height="315" src="https://www.youtube.com/embed/IRmoD1BJFHM" frameborder="0" allowfullscreen></iframe>
            <br><br>
             <iframe width="560" height="315" src="https://www.youtube.com/embed/HXLc0rLK6_g" frameborder="0" allowfullscreen></iframe>

            <br><br>
            <center>
              <div style="width:70%">
                This program uses computer vision searches for fires by searching for intense brightness and fire-like movement in the video. It then monitors the rate of growth and current size of the fire to determine on how threatening a fire is becoming and gives a pre-emptive warning (via phone notification) to nearby individuals if it gets to a certain size / surpasses a certain growth rate.


                <br><br>
                This was designed for PennApps Fall 2018 where it placed in top 30 and won the DocuSign API prize.
                <br><br>
                

              </div>
            </center>

            <br><br>
            Tactile Vision Shirt
            <br><br>
            <img src= "images/medhacks0.jpg" width="20%" height="auto"></a>     
            <img src= "images/medhacks4.jpg" width="20%" height="auto"></a>
            <br><br>
            <img src= "images/medhacks1.jpg" width="25%" height="auto"></a>    
            <img src= "images/medhacks2.jpg" width="35%" height="auto"></a>
            <img src= "images/medhacks3.jpg" width="25%" height="auto"></a>
            

            <br><br>
            <center>
              <div style="width:70%">
                Made a hardware hack that recreated the sense of sight to the blind through the use of tactile touch. Based on the concept of graphesthesia, this project utilized a grid of 5 x 5 vibration motors sewed on the back of a shirt and two webcams to locate the object in order to vibrate the object's 3D location on the user's back. Implemented depth maps through stereo vision and object detection through YOLO neural network architecture (MobileNet) to be used on a Raspberry Pi. Also designed a downsampling algorithm to preserve semantic and spatial information from a webcam with 300x400 resolution to 5x5 resolution for the vibration motor grid.


                <br><br>
                Won 1st place overall at MedHacks Fall 2019, along with winning the Global Management of Chronic Disease track.
                <br><br>
                

              </div>
            </center>


          <br><br>
           Automatic decision-based workflow generator via speech processing (Autoflow)
            <br><br>
            
              <a href="https://devpost.com/software/auto-flow" target="_blank"><img src= "images/autoflow0.png" width="35%"><img src= "images/autoflow1.jpeg" width="30%">
                <br>
                <img src= "images/autoflow3.png" width="60%">
                <br><br>
                <img src= "images/autoflow2.png" width="60%"></a>

            <br><br>
            <center>
              <div style="width:70%">
                Made an app that can create an automated workflow of APIs produced through spoken speech. This project takes in a spoken query via speech to text and uses the query to build an automated decision tree that handles the workflow's use various types of APIs (via Standard Library) under dynamic and conditional situations. Created an algorithm that turns user's input queries into a automated decision tree through parts of speech tagging, as well as designed an algorithm that detect redundant queries and workflows by using cosine similarity. Also helped designed the algorithm that represents the automated workflow tree in the form of JSON files.
                <br><br>
                Placed in the finalist round at HackPrinceton Fall 2019 as well as winning the Standard Library prize.
                <br><br>
                

              </div>
            </center>
            <br><br>
            Deepfake algorithm (CycleGANs)

            
            <div class="lazy slider" data-sizes="50vw" style="height:410px;">
              <div>
                <br><br><br>
                Obama's face swapped with Trump's
                <img data-lazy="images/df_0.jpg" data-srcset="images/df_0.jpg 650w, images/df_0.jpg 960w" data-sizes="100vw">
                (Press the left or right arrows to change photos)
              </div>
              <div>
                <br><br><br>
                Trump's face swapped with Putin's
                <img data-lazy="images/df_1.jpg" data-srcset="images/df_1.jpg 650w, images/df_1.jpg 960w" data-sizes="100vw">

              </div>
               <div>
                <br><br>
                Trump's face swapped with Putin's
                <img data-lazy="images/df_2.jpg" data-srcset="images/df_2.jpg 650w, images/df_2.jpg 960w" data-sizes="100vw">
              </div>
               <div>
                <br><br><br>
                Putin's face swapped with Trump's
                <img data-lazy="images/df_3.jpg" data-srcset="images/df_3.jpg 650w, images/df_3.jpg 960w" data-sizes="100vw">
              </div>
              <div>
                <br><br><br>
                Obama's face swapped with Trump's
                <img data-lazy="images/df_4.jpg" data-srcset="images/df_4.jpg 650w, images/df_4.jpg 960w" data-sizes="100vw">
              </div>
              <div>
                A girl's face (Shuka) swapped with Trump's
                <img data-lazy="images/df_5.jpg" data-srcset="images/df_5.jpg 650w, images/df_5.jpg 960w" data-sizes="100vw">
              </div>
              <div>
                <br><br><br>
                A girl's face (Shuka) swapped with Trump's
                <img data-lazy="images/df_6.jpg" data-srcset="images/df_6.jpg 650w, images/df_6.jpg 960w" data-sizes="100vw">
              </div>
              <div>
                Arisa's face swapped with Shuka's
                <img data-lazy="images/df_7.jpg" data-srcset="images/df_7.jpg 650w, images/df_7.jpg 960w" data-sizes="100vw">
              </div>
              <div>
                Arisa's face swapped with Shuka's
                <img data-lazy="images/df_11.jpg" data-srcset="images/df_11.jpg 650w, images/df_11.jpg 960w" data-sizes="100vw">
              </div>
              <div>
                Shuka's face swapped with Arisa
                <img data-lazy="images/df_9.jpg" data-srcset="images/df_9.jpg 650w, images/df_9.jpg 960w" data-sizes="100vw">
              </div>
              <div>
                <br><br><br>
                Arisa's face swapped with Shuka's
                <img data-lazy="images/df_10.jpg" data-srcset="images/df_10.jpg 650w, images/df_10.jpg 960w" data-sizes="100vw">
              </div>
 
       
            </div>


            <center>

              <div style="width:70%;">
                Made an AI that can transform the face of anyone into image with another person's face (in this case, Obama -> Trump and Trump -> Putin). The algorithm was implemented through a CycleGAN to change the image domain of facial data along affline warping and transformation to extract the original face and replace with a generated face of another subject. Extracted training data using a web scaper along with implementing several different image preproccessing scripts. 
                

              </div>
            </center>

            </center>
            <br><br>
            Hand-drawn graphs to LATEX translator
            <br><br>
            
              <a href="https://github.com/aliang8/Paper2LaTeX" target="_blank"><img src= "images/graph0.png" width="340" height="200"><img src= "images/graph1.png" width="340" height="200"></a>

            <br><br>
            <center>
              <div style="width:70%">
                Implented computer vision and machine learning in order to analyze a hand-drawn graph on paper, seach and analyze shapes such as nodes and lines, and translate them into a more tidy and neat LATEX graph. Helped plan computer vision algorithms to detect nodes and lines while worked on optical character recognitiion and arrow direction detection.
                <br><br>
                Placed in top 30 hacks in PennApps Winter 2017 hackathon.
                <br><br>
                

              </div>
            </center>

             <br><br>
            CNN-LSTM Video Classifier 
            <br><br>
            
              <iframe width="560" height="315" src="https://www.youtube.com/embed/yzEKe9Eoa0Q" frameborder="0" allowfullscreen></iframe>

            <br><br>
            <center>
              <div style="width:70%">
               This video was a result of training a CNN-LSTM video classifer on a small dataset. By analyzing a series of frames in each video, the classifer is able to recognize actions going on in the video. 

		By having CNN encoder taking in the frames, the frames are parsed to be analyzed by the LSTM which analyzes the temporial structure of the frames and decodes the output to get a classification of the video.  

              </div>
            </center>



             <br><br>
            Video Panorama Generator 
            <br><br>
            
              <iframe width="560" height="315" src="https://www.youtube.com/embed/hVynpc-li1w" frameborder="0" allowfullscreen></iframe>

            <br><br>
            <center>
              <div style="width:70%">
                This video is a result from a python script I made that stitches two videos that were recorded from two cameras looking at the same scene at different angle to create video panoramas. Uses the SIFT algorithm to find similar points and calculate a transformation matrix with OpenCV to stitch the videos to together
                This video is the end result. May work on improving making the seams less visible when I get the time.
                

              </div>
            </center>


            <br><br>
            Surround and Capturing Adversarial Agents through Decentralized Multi-Agent Intelligence
            <br><br>
            
              <a href="TTUResearch0.pdf"><img src= "images/swarm0.png" width="440" height="340"></a>

            <br><br>
            <center>
              <div style="width:70%">
                Designed a multi-agent intelligence algorithm (swarm intelligence) where a team of ally agents work together to surround and capture a fleeing adversarial agent. This allowed agents to search for interest points to pursue and surround enemy agent with individual behavior, but minimal communication to work together as an agent team for capturing an adversarial agent.
                <br><br>
                Abstract was accepted to the National Conference On Undergraduate Research (NCUR 2017) at the Memphis, Tennessee.
                <br><br>
                <a href="TTUResearch0.pdf" target="_blank" style="color: #2E9AFE">You can see the research poster here</a>
                <br><br>
                

              </div>
            </center>


            <br><br>
            Drawing to Website Generator using Computer Vision
            <br><br>
            
              <iframe width="560" height="315" src="https://www.youtube.com/embed/7QbGQF6JS4g" frameborder="0" allowfullscreen></iframe>

            <br><br>
            <center>
              <div style="width:70%">
                Made an AI website generator that uses computer vision to recognize shapes and position formats of your drawing and turns it into your own website. You would draw a picture of your website, upload your drawing, specify the topic of your website, and the program will extract information from multiple sources about that topic and use the information to populate your generated website.
 
              </div>
            </center>


            <br><br>
            White Space detector



            <div class="lazy slider" data-sizes="50vw" style="height:410px;">
              <div>
                <br><br><br>
                <img src= "images/pru0.png" width="45%" height="auto">
                (Press the left or right arrows to change photos)
              </div>
              <div>
                <br><br><br>
                <img src= "images/pru1.png" width="45%" height="auto">
                
              </div>
              <div>
                <br><br><br>
                <img src= "images/pru2.png" width="45%" height="auto">
                
              </div>
              <div>
                <br><br><br>
                <img src= "images/pru3.png" width="45%" height="auto">
                
              </div>
              <div>
                <br><br><br>
                <img src= "images/pru4.png" width="45%" height="auto">
                
              </div>
              <div>
                <br><br><br>
                <img src= "images/pru5.png" width="45%" height="auto">

                
              </div>
            </div>

            <center>
            
              <div style="width:70%">
                Implemented canny edge detection and morphological transform in order to search for whitespace or less detailed spaces with an image to write informative headlines or text on to it. Removes the need for content writers to go through endless of photos and manually edit and add text, saving content writers a lot of time.
                <br><br><br>
                (This was made for a company, and permission was given to post these)
                <br><br>
                

              </div>
            </center>



            <br><br>
            Road Segmentation For Autonomous Vehicles
            <br><br>
            <a href="https://github.com/vanstorm9/urban-road-recognition" target="_blank"><img src= "images/road_0.png" width="560" height="270"></a>
            <br><br>
            <center>
              <div style="width:70%">
                Designed a simple script that uses histogram backprojection along with morphological transform in order detect a road in a scene despite any large amounts of noise in the picture. Made to be implemented in the DriveAI project, an initiative for open-source autonomous vehicles.
                <br><br>
                

              </div>
            </center>

            <br><br>
            Rutgers Navigation System
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/Pt3fnORMzA8" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Uses the A* algorithm in order to find the shortest path from point A to point B within Rutgers campus (to find buildings, bus stops, dining halls, etc) (currently Busch campus) as well as kinematics equations to find the time of the walk. Calcuates both the path and information of bus route and walking route to help one determine the more efficient way of getting to a destination (takes into account the bus route, average walking and bus driving speed, and distance between each places)
                <br><br>
                

              </div>
            </center>


            <br><br><br><br><br><br><br><br><br><br><br><br>
            <div class="roundedImageHCI">&nbsp;</div>
            <font size="5">Human-Computer Interaction</font>
            <br><br>
            <br><br>
            Force VR Gauntlet
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/PMwATLDO3yU" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Used a motor (with rope tethered to arm) controlled by the Arduino to make a virtual reality wearable that allows the wearer to experience physical forces. Whenever the user moves / swings his arm in the VR game and a hit is detected, the Arduino recieves that information and pulls on the arm with the motor tethered to arm, creating a locking effect and imitation of a physical force. It was designed to solve the problem of lack of physical forces in virtual reality (say you have a sword. You hit an object and your arm stops because it cannot go through an object, but in VR, your arm would simply phase through the object). With this developped a little bit more, actual sword combat will be finally possible in virtual reality.
                <br><br>
              </div>
            </center>

            <br><br>
            Kinect Helper
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/svzljeO0i2k" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Used the Kinect to create a workspace that is able to sense the presence of its owner. By using object detection, the
                Kinect is able to call a bash script that gives the Kinect control of the computer based on user input, creating a smart workspace. In this case the Kinect can sense when you get up from your chair and will automatically shut off the screen, this saves power and is more convenient than having to do it manually. We also included other features such as the ability to change the brightness of the screen using a hand held up to the sensor. Any other features can be added to the program and controlled with a hand held up to the Kinect.
                <br><br>
                
              </div>
            </center>

            <br><br>
            Leaptop
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/SqsctnDnpdc" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Used the Leap Motion in order to control some computer functions using just hand gestures. This includes opening up a browser, closing a browser, putting the laptop to sleep, and locking it using hand movements. This was made in Python which was also controlling a bash script.
                <br><br>
              </div>
            </center>
            <br><br>
            Hologram Pyramid CAD program
            <br><br>

            <iframe width="560" height="315" src="https://www.youtube.com/embed/Kmb2xlh2wm8" frameborder="0" allowfullscreen></iframe><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/1y1NO4AM0k4" frameborder="0" allowfullscreen></iframe>

            <br><br>
            <center>
              <div style="width:70%">
                
                EDIT: I know, the quality of the video isn't that good, so I will upload another video in the near future. <br><br>

              A friend and I made a projector that uses pepper's ghost to create a hologram like image in an acrylic pyramid. Used Unity for the models and projection scene along with Leap Motion to give us the ability to control the orientation of the model. If developped more, it would serve as a CAD program for designers, artists, and engineers so that they can view their models and control them in 3D space.
                <br><br>
              </div>
            </center>
            <br><br>
            Hologram Emotion Face Control
            <br><br>

            <a href="https://github.com/megatran/EmotionRecognitionOnHologram" target="_blank"><img src= "images/emotionPyramid.jpg" width="560" height="315"></a>

            <br><br>
            <center>
              <div style="width:70%">
                
              We use the hologram pyramid to project a face into the pyramid. By using convolution neural networks and a server, we were able to change the the hologram face's emotion from happy to angry depending on what the emotion of someone else on another computer. We hope to work on full face control in the future.
                <br><br>
              </div>
            </center>
            <br><br>
            <font size="5">Game Development</font>
            <br><br>

            Myo Multiplayer Sword Fighting Game
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/6phNDQvRKuU" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Created a sword fighting multiplayer game wheremultiple people can log into the same server and engage sword combat with each other. Programmedthe Myo so that users are able to preform sword attacks through sword cutting like hand gestures. It was designed as a test game that may be implemented with the Force VR Gauntlet.
                <br><br>
              </div>
            </center>
            <br><br>
            Twitch Based Jenga Multiplayer Game
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/v_voaCJqpFA" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                Created a Twitch based collaboration game in Jenga where players can log into the same server and play Jenga with each other while the audience is able to interfer with the gameplay in Unity by tying in commands on Twitch. Created this as my first Unity game development project.
                <br><br>
              </div>
            </center>
             <br><br>
            NES Zelda recreaton
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/StfCZy4-NRA" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                A game I made in Visual Basic, which is a small segment recreation of the Legend of Zelda for NES.
              </div>

            <br><br>
            <font size="5">Web Development</font>
            

            <br><br>
            Dodo-Matchmaker 
            <div class="lazy slider" data-sizes="50vw" style="height:410px;">
              <div>
                <br><br>
                Quick matchup feature demo
                <video  width="100%" height="auto" controls>
                  <source src= "video/dodo-0.mp4" type="video/mp4">
                </video>
                (Press the left or right arrows to change photos)
              </div>
              <div>
                <br><br>
                Private host room feature demo
                <video  width="100%" height="auto" controls>
                  <source src= "video/dodo-1.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <center>
              <div style="width:70%">
                An online code matchmaker website designed for players of the game, "Animal Crossing: New Horizons". Matches different players by distributing online access codes (dodo-code) to different players based on specific conditions. Also contains Facebook verification to help improve responoses to and discourage scams online in the game. Made in Python Flask, socketio, Javascript, HTML, and CSS.

              </div>


            <br><br>
            Animal Crossing GIF Storymaker
            <br><br>
            <video  height="380px" width="auto" controls>
              <source src= "video/gif-demo.mp4" type="video/mp4">
            </video>
            <br><br>
            <center>
              <div style="width:70%">
                A web app designed for the game "Animal Crossing: New Horizons". Allows you to generate your own cutscenes based on a particular scene of the character, Isabelle, doing daily morning annoucnements. Uses frames from in game and allows you to mix different reactions in different combinations along with dialogue text customization. Extracted frames from the game and pre-processed using Python before being used in the app.
                <br><br>
                <a href="http://ac-storymaker.github.io/" target="_blank"  style="color: #2E9AFE">Check out the site</a>
              </div>
            <br><br>

            <br><br>
            WannaStudy
            <br><br>
            <center>
              <div style="width:70%">
                A social networking forum designed to help create academic community and supports the circulation of academic information within colleges in order for students to ask and answer questions, form study groups, study online through Google Hangouts (button integrated on site), and share information.
              </div>
            <br><br>
            Rutgers Roommate Search Engine
            <br><br>
            <center>
              <div style="width:70%">
                Account system website designed to have users register themselves to a roommate search engine in order to help them or others to find the perfect roommate. Includes an advanced search section, a list of roommates filtered through advance search, profile pages for each user, and roommate lists to add potential roommates on their list. 
                
              </div>
            <br><br>
            PVHS GPA Calculator
            <br><br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/to74NTcSUtE" frameborder="0" allowfullscreen></iframe>
            <br><br>
            <center>
              <div style="width:70%">
                An old website with PHP/SQL account systems that helps students calculate their GPA (marking period, end-of-year, and current)as well final grad. 
              </div>
            <br><br>


            <br><br><br><br><br><br><br><br><br><br>
          </div>
	 </div>
	
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
		<script src = "js/bootstrap.js"></script>
  <script src="js/slick.js" type="text/javascript" charset="utf-8"></script>

	</body>


	
</html>
